{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\u001b[33mProcessed with HashDetector[4985455664], similar_groups count: 0\u001b[0m\n",
      "Detecting duplicates using perceptual hash, precision: 8\n",
      "images cnt: 4995\n",
      "Found 4330 unique hashes\n",
      "Found 3683 unique images, 647 similar groups\n",
      "\n",
      "[3683, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Saving description to /Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]/thumbnail/descriptor_HashDetector_20240511225301.txt\n",
      "\u001b[33munique count: 3683\u001b[0m\n",
      "\u001b[33mProcessed with ORBDetector[4985456720], similar_groups count: 647\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from imagehash import phash\n",
    "from itertools import combinations\n",
    "from typing import List, Set, Tuple\n",
    "from termcolor import colored\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageDescriptor:\n",
    "    def __init__(self, unique_images: Set[Path], similar_groups: List[List[Path]]):\n",
    "        self.unique_images = unique_images\n",
    "        self.similar_groups = similar_groups\n",
    "        print([len(unique_images)] + [len(sub_array) for sub_array in similar_groups])\n",
    "\n",
    "    def serialize(self, filepath: str):\n",
    "        \"\"\"将描述信息保存为文本文件\"\"\"\n",
    "        print(f\"Saving description to {filepath}\")\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(\"Unique Images:\\n\")\n",
    "            for image in self.unique_images:\n",
    "                file.write(f\"{image.name}\\n\")\n",
    "            file.write(\"\\nSimilar Groups:\\n\")\n",
    "            for group in self.similar_groups:\n",
    "                file.write(f\"Group:\\n\")\n",
    "                for image in group:\n",
    "                    file.write(f\"{image.name}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "class HashDetector:\n",
    "    def __init__(self, precision: int):\n",
    "        self.precision = precision\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        print(f\"Detecting duplicates using perceptual hash, precision: {self.precision}\\nimages cnt: {len(images)}\")\n",
    "        hash_dict = {}\n",
    "        \n",
    "        # 添加tqdm进度条\n",
    "        for image_path in tqdm(images, desc=\"Hashing images\"):\n",
    "            with Image.open(image_path) as img:\n",
    "                # 计算图片的perceptual hash\n",
    "                img_hash = phash(img.convert(\"L\").resize((self.precision, self.precision)))\n",
    "                if img_hash in hash_dict:\n",
    "                    hash_dict[img_hash].append(image_path)\n",
    "                else:\n",
    "                    hash_dict[img_hash] = [image_path]\n",
    "                    \n",
    "        print(f\"Found {len(hash_dict)} unique hashes\")\n",
    "        unique_images = set()\n",
    "        similar_groups = []\n",
    "        \n",
    "        # 添加tqdm进度条\n",
    "        for paths in tqdm(hash_dict.values(), desc=\"Grouping images\"):\n",
    "            if len(paths) == 1:\n",
    "                unique_images.add(paths[0])\n",
    "            else:\n",
    "                similar_groups.append(paths)\n",
    "                \n",
    "        print(f\"Found {len(unique_images)} unique images, {len(similar_groups)} similar groups\\n\")\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "class ORBDetector:\n",
    "    def __init__(self, nfeatures: int, threshold: float):\n",
    "        self.nfeatures = nfeatures\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        keypoints_dict = {img: self._extract_features(img) for img in images}\n",
    "        similar_groups = []\n",
    "        unique_images = set(images)\n",
    "        \n",
    "        # 直接计算组合数，而不生成组合列表\n",
    "        total_combinations = len(images) * (len(images) - 1) // 2\n",
    "        \n",
    "        # 使用tqdm直接包装组合迭代器\n",
    "        progress_bar = tqdm(combinations(images, 2), total=total_combinations, desc=\"Matching features\")\n",
    "        for img1, img2 in progress_bar:\n",
    "            kp1, des1 = keypoints_dict[img1]\n",
    "            kp2, des2 = keypoints_dict[img2]\n",
    "            if des1 is not None and des2 is not None:\n",
    "                if self._match_features(des1, des2) > self.nfeatures * self.threshold:\n",
    "                    similar_groups.append([img1, img2])\n",
    "                    unique_images.discard(img1)\n",
    "                    unique_images.discard(img2)\n",
    "\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "    def _extract_features(self, image_path: Path):\n",
    "        orb = cv2.ORB_create(self.nfeatures)\n",
    "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        return orb.detectAndCompute(img, None)\n",
    "\n",
    "    def _match_features(self, des1, des2):\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        return len(matches)\n",
    "\n",
    "class ImageDeduplicator:\n",
    "    def __init__(self, directory: str):\n",
    "        self.directory = directory\n",
    "        self.detectors = [\n",
    "            HashDetector(8),\n",
    "            ORBDetector(500, 0.7)\n",
    "        ]\n",
    "\n",
    "    def deduplicate(self):\n",
    "        images = [path for path in Path(self.directory).glob(\"*\") if path.suffix.lower() in [\".jpg\", \".png\"]]\n",
    "        descriptor = ImageDescriptor(set(), [])\n",
    "\n",
    "        for detector in self.detectors:\n",
    "            print(colored(f\"Processed with {type(detector).__name__}[{id(detector)}], similar_groups count: {len(descriptor.similar_groups)}\", \"yellow\"))\n",
    "            result = detector.detect(images)\n",
    "            descriptor.unique_images.update(result.unique_images)\n",
    "            descriptor.similar_groups.extend(result.similar_groups)\n",
    "\n",
    "            # 序列化当前描述对象\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            filepath = f\"{self.directory}/descriptor_{type(detector).__name__}_{timestamp}.txt\"\n",
    "            descriptor.serialize(filepath)\n",
    "\n",
    "            print(colored(f\"unique count: {len(descriptor.unique_images)}\", \"yellow\"))\n",
    "        \n",
    "        return descriptor\n",
    "\n",
    "# Example usage\n",
    "# deduplicator = ImageDeduplicator(\"/Users/chenweichu/dev/data/test\")\n",
    "deduplicator = ImageDeduplicator(\"/Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]/thumbnail\")\n",
    "\n",
    "final_descriptor = deduplicator.deduplicate()\n",
    "print(f\"Final unique images count: {len(final_descriptor.unique_images)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-img-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
