{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mProcessed with HashDetector[6260114672], similar_groups count: 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing images: 100%|██████████| 4995/4995 [02:24<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mFound 4330 unique hashes\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping images: 100%|██████████| 4330/4330 [00:00<00:00, 388511.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mFound 3683 unique images, 647 similar groups\u001b[0m\n",
      "\u001b[37mSaving description to /Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]/descriptor_HashDetector_20240512140338.txt\u001b[0m\n",
      "\u001b[33mAfter processed, similar_groups count: 647, unique count: 3683\n",
      "\u001b[0m\n",
      "\u001b[33mProcessed with ORBDetector[6260116928], similar_groups count: 647\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [00:28<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mSaving description to /Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]/descriptor_ORBDetector_20240512140407.txt\u001b[0m\n",
      "\u001b[33mAfter processed, similar_groups count: 377, unique count: 4255\n",
      "\u001b[0m\n",
      "\u001b[37mFinal unique images count: 4255\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from imagehash import phash\n",
    "from itertools import combinations\n",
    "from typing import List, Set, Tuple\n",
    "from termcolor import colored\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageDescriptor:\n",
    "    def __init__(self, unique_images: Set[Path], similar_groups: List[List[Path]]):\n",
    "        self.unique_images = unique_images\n",
    "        self.similar_groups = similar_groups\n",
    "        # tqdm.write(colored(f\"ImageDescripter constructed: {[len(unique_images)] + [len(sub_array) for sub_array in similar_groups]}\", \"white\"))\n",
    "\n",
    "    def serialize(self, filepath: str):\n",
    "        \"\"\"将描述信息保存为文本文件\"\"\"\n",
    "        tqdm.write(colored(f\"Saving description to {filepath}\", \"white\"))\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(\"Unique Images:\\n\")\n",
    "            for image in self.unique_images:\n",
    "                file.write(f\"{image.name}\\n\")\n",
    "            file.write(\"\\nSimilar Groups:\\n\")\n",
    "            for group in self.similar_groups:\n",
    "                file.write(f\"Group:\\n\")\n",
    "                for image in group:\n",
    "                    file.write(f\"{image.name}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "class HashDetector:\n",
    "    def __init__(self, precision: int):\n",
    "        self.precision = precision\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        # tqdm.write(colored(f\"Detecting duplicates using perceptual hash, precision: {self.precision}\\nimages cnt: {len(images)}\", \"white\"))\n",
    "        hash_dict = {}\n",
    "        \n",
    "        # 添加tqdm进度条\n",
    "        for image_path in tqdm(images, desc=\"Hashing images\"):\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    # 计算图片的perceptual hash\n",
    "                    img_hash = phash(img.convert(\"L\").resize((self.precision, self.precision)))\n",
    "                    if img_hash in hash_dict:\n",
    "                        hash_dict[img_hash].append(image_path)\n",
    "                    else:\n",
    "                        hash_dict[img_hash] = [image_path]\n",
    "            except Exception as e:\n",
    "                tqdm.write(colored(f\"Error processing {image_path}: {e}\", \"red\"))\n",
    "                    \n",
    "        tqdm.write(colored(f\"Found {len(hash_dict)} unique hashes\", \"white\"))\n",
    "        unique_images = set()\n",
    "        similar_groups = []\n",
    "        \n",
    "        # 添加tqdm进度条\n",
    "        for paths in tqdm(hash_dict.values(), desc=\"Grouping images\"):\n",
    "            if len(paths) == 1:\n",
    "                unique_images.add(paths[0])\n",
    "            else:\n",
    "                similar_groups.append(paths)\n",
    "                \n",
    "        tqdm.write(colored(f\"Found {len(unique_images)} unique images, {len(similar_groups)} similar groups\", \"white\"))\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "class ORBDetector:\n",
    "    def __init__(self, nfeatures: int, threshold: float):\n",
    "        self.nfeatures = nfeatures\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        # tqdm.write(colored(f\"Detecting duplicates using ORB, nfeatures: {self.nfeatures}, threshold: {self.threshold}, images cnt: {len(images)}\", \"white\"))\n",
    "        keypoints_dict = {img: self._extract_features(img) for img in images}\n",
    "        similar_groups = []\n",
    "        unique_images = set(images)\n",
    "        \n",
    "        # 直接计算组合数，而不生成组合列表\n",
    "        combines = combinations(images, 2)\n",
    "        \n",
    "        # 使用tqdm直接包装组合迭代器\n",
    "        # for img1, img2 in tqdm(combines, total=len(images) * (len(images) - 1) // 2, desc=\"Matching features\"):\n",
    "        for img1, img2 in combines:\n",
    "            kp1, des1 = keypoints_dict[img1]\n",
    "            kp2, des2 = keypoints_dict[img2]\n",
    "            if des1 is not None and des2 is not None:\n",
    "                if self._match_features(des1, des2) > self.nfeatures * self.threshold:\n",
    "                    similar_groups.append([img1, img2])\n",
    "                    unique_images.discard(img1)\n",
    "                    unique_images.discard(img2)\n",
    "\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "    def _extract_features(self, image_path: Path):\n",
    "        orb = cv2.ORB_create(self.nfeatures)\n",
    "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        return orb.detectAndCompute(img, None)\n",
    "\n",
    "    def _match_features(self, des1, des2):\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        return len(matches)\n",
    "\n",
    "class ImageDeduplicator:\n",
    "    def __init__(self, directory: str):\n",
    "        if not os.path.exists(directory) or not os.path.isdir(directory):\n",
    "            tqdm.write(colored(f\"Directory {directory} not valid\", \"red\"))\n",
    "            self.directory = None\n",
    "            return\n",
    "        thumbnail = Path(f\"{directory}/thumbnail\")\n",
    "        if not os.path.exists(thumbnail) or not os.path.isdir(thumbnail):\n",
    "            tqdm.write(colored(f\"Directory {thumbnail} not valid, create thumbnails first\", \"red\"))\n",
    "            self.directory = None\n",
    "            return\n",
    "        self.directory = directory\n",
    "        self.detectors = [\n",
    "            HashDetector(8),\n",
    "            ORBDetector(1000, 0.7)\n",
    "        ]\n",
    "\n",
    "    def deduplicate(self):\n",
    "\n",
    "        if self.directory is None:\n",
    "            tqdm.write(colored(\"Directory not valid\", \"red\"))\n",
    "            return\n",
    "        thumbails = [file for file in Path(f\"{self.directory}/thumbnail\").glob('*') if not file.name.startswith('.') and file.suffix.lower() in [\".jpg\", \".png\"]]\n",
    "        descriptor = ImageDescriptor(set(), [thumbails])\n",
    "\n",
    "        for detector in self.detectors:\n",
    "            tqdm.write(colored(f\"Processed with {type(detector).__name__}[{id(detector)}], similar_groups count: {len(descriptor.similar_groups)}\", \"yellow\"))\n",
    "            new_descripter = ImageDescriptor(descriptor.unique_images, [])\n",
    "            if isinstance(detector, ORBDetector):\n",
    "                for group_of_img in tqdm(descriptor.similar_groups):\n",
    "                    result = detector.detect(group_of_img)\n",
    "                    new_descripter.unique_images.update(result.unique_images)\n",
    "                    new_descripter.similar_groups.extend(result.similar_groups)\n",
    "            else:\n",
    "                for group_of_img in descriptor.similar_groups:\n",
    "                    result = detector.detect(group_of_img)\n",
    "                    new_descripter.unique_images.update(result.unique_images)\n",
    "                    new_descripter.similar_groups.extend(result.similar_groups)\n",
    "            descriptor = new_descripter\n",
    "            # 序列化待序列化的描述对象\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            filepath = f\"{self.directory}/descriptor_{type(detector).__name__}_{timestamp}.txt\"\n",
    "            descriptor.serialize(filepath)\n",
    "            tqdm.write(colored(f\"After processed, similar_groups count: {len(descriptor.similar_groups)}, unique count: {len(descriptor.unique_images)}\\n\", \"yellow\"))\n",
    "        \n",
    "        return descriptor\n",
    "\n",
    "# Example usage\n",
    "# deduplicator = ImageDeduplicator(\"/Users/chenweichu/dev/data/test_副本\")\n",
    "deduplicator = ImageDeduplicator(\"/Volumes/192.168.1.173/pic/陈都灵_503[167_MB]\")\n",
    "deduplicator = ImageDeduplicator(\"/Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]\")\n",
    "\n",
    "final_descriptor = deduplicator.deduplicate()\n",
    "if final_descriptor is None:\n",
    "    tqdm.write(colored(\"Deduplication failed\", \"red\"))\n",
    "else:\n",
    "    tqdm.write(colored(f\"Final unique images count: {len(final_descriptor.unique_images)}\", \"white\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-img-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
