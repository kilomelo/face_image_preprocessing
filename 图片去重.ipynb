{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStart deduplicating images in /Users/chenweichu/dev/data/test_副本, collect thumbnail images.\u001b[0m\n",
      "\u001b[32mCollected 85 thumbnail images.\u001b[0m\n",
      "\u001b[33mProcessed with HashDetector[5941338512], unique images: 28 similar_groups count: 22\u001b[0m\n",
      "\u001b[33mProcessed with HashDetector[5941339568], unique images: 30 similar_groups count: 21\u001b[0m\n",
      "\u001b[34mGroup 3 is removed by HashDetector\u001b[0m\n",
      "\u001b[37mSaving description to [/Users/chenweichu/dev/data/test_副本/descriptor_0_HashDetector_20240513121623.txt]\u001b[0m\n",
      "\u001b[33mProcessed with ORBDetector[5941339664], unique images: 30 similar_groups count: 57\u001b[0m\n",
      "\u001b[34mGroup 0 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 1 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 5 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 6 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 7 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 10 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 15 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 16 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 19 is removed by ORBDetector\u001b[0m\n",
      "\u001b[37mSaving description to [/Users/chenweichu/dev/data/test_副本/descriptor_1_HashDetector_20240513121623.txt]\u001b[0m\n",
      "\u001b[33mProcessed with ORBDetector[5941339760], unique images: 50 similar_groups count: 44\u001b[0m\n",
      "\u001b[34mGroup 26 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 27 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 28 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 29 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 33 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 34 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 35 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 36 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 41 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 42 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 43 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 51 is removed by ORBDetector\u001b[0m\n",
      "\u001b[34mGroup 52 is removed by ORBDetector\u001b[0m\n",
      "\u001b[37mSaving description to [/Users/chenweichu/dev/data/test_副本/descriptor_2_ORBDetector_20240513121623.txt]\u001b[0m\n",
      "\u001b[37mSaving description to [/Users/chenweichu/dev/data/test_副本/descriptor_3_ORBDetector_20240513121623.txt]\u001b[0m\n",
      "\u001b[37mFinal unique images count: 50\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 迭代编号：2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from imagehash import phash\n",
    "from itertools import combinations\n",
    "from typing import List, Set, Tuple\n",
    "from termcolor import colored\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "class ImageDescriptor:\n",
    "    def __init__(self, unique_images: Set[Path], similar_groups: List[List[Path]]):\n",
    "        self.unique_images = unique_images\n",
    "        self.similar_groups = similar_groups\n",
    "        self.group_removed_by_next_detector = []  # 新增：存储每个相似组的哈希值列表 迭代编号：2\n",
    "\n",
    "    def serialize(self, filepath: str):\n",
    "        \"\"\"将描述信息保存为文本文件\"\"\"\n",
    "        tqdm.write(colored(f\"Saving description to [{filepath}]\", \"white\"))\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(\"Unique Images:\\n\")\n",
    "            for image in self.unique_images:\n",
    "                file.write(f\"{image.name}\\n\")\n",
    "            file.write(\"\\nSimilar Groups:\\n\")\n",
    "            for group, is_removed in zip(self.similar_groups, self.group_removed_by_next_detector):\n",
    "                group_header = \"Group:removed\\n\" if is_removed else \"Group:\\n\"  # 修改：追踪是否删除 迭代编号：2\n",
    "                file.write(group_header)\n",
    "                for image in group:\n",
    "                    file.write(f\"{image.name}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "class HashDetector:\n",
    "    def __init__(self, precision: int):\n",
    "        self.precision = precision\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        # tqdm.write(colored(f\"Detecting duplicates using perceptual hash, precision: {self.precision}\\nimages cnt: {len(images)}\", \"white\"))\n",
    "        hash_dict = {}\n",
    "        # for image_path in tqdm(images, desc=\"Hashing images\"):\n",
    "        for image_path in images:\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img_hash = phash(img.convert(\"L\").resize((self.precision, self.precision)))\n",
    "                    if img_hash in hash_dict:\n",
    "                        hash_dict[img_hash].append(image_path)\n",
    "                    else:\n",
    "                        hash_dict[img_hash] = [image_path]\n",
    "            except Exception as e:\n",
    "                tqdm.write(colored(f\"Error processing {image_path}: {e}\", \"red\"))\n",
    "                    \n",
    "        # tqdm.write(colored(f\"Found {len(hash_dict)} unique hashes\", \"white\"))\n",
    "        unique_images = set()\n",
    "        similar_groups = []\n",
    "        # for paths in tqdm(hash_dict.values(), desc=\"Grouping images\"):\n",
    "        for paths in hash_dict.values():\n",
    "            if len(paths) == 1:\n",
    "                unique_images.add(paths[0])\n",
    "            else:\n",
    "                similar_groups.append(paths)\n",
    "                \n",
    "        # tqdm.write(colored(f\"Found {len(unique_images)} unique images, {len(similar_groups)} similar groups\", \"white\"))\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "class ORBDetector:\n",
    "    def __init__(self, nfeatures: int, threshold: float):\n",
    "        self.nfeatures = nfeatures\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, images: List[Path]) -> ImageDescriptor:\n",
    "        # tqdm.write(colored(f\"Detecting duplicates using ORB, nfeatures: {self.nfeatures}, threshold: {self.threshold}, images cnt: {len(images)}\", \"white\"))\n",
    "        keypoints_dict = {img: self._extract_features(img) for img in images}\n",
    "        similar_groups = []\n",
    "        unique_images = set(images)\n",
    "        combines = combinations(images, 2)\n",
    "        for img1, img2 in combines:\n",
    "            kp1, des1 = keypoints_dict[img1]\n",
    "            kp2, des2 = keypoints_dict[img2]\n",
    "            if des1 is not None and des2 is not None:\n",
    "                if self._match_features(des1, des2) > self.nfeatures * self.threshold:\n",
    "                    similar_groups.append([img1, img2])\n",
    "                    unique_images.discard(img1)\n",
    "                    unique_images.discard(img2)\n",
    "\n",
    "        return ImageDescriptor(unique_images, similar_groups)\n",
    "\n",
    "    def _extract_features(self, image_path: Path):\n",
    "        orb = cv2.ORB_create(self.nfeatures)\n",
    "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        return orb.detectAndCompute(img, None)\n",
    "\n",
    "    def _match_features(self, des1, des2):\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        return len(matches)\n",
    "\n",
    "class ImageDeduplicator:\n",
    "    def __init__(self, directory: str):\n",
    "        if not os.path.exists(directory) or not os.path.isdir(directory):\n",
    "            tqdm.write(colored(f\"Directory {directory} not valid\", \"red\"))\n",
    "            self.directory = None\n",
    "            return\n",
    "        thumbnail = Path(f\"{directory}/thumbnail\")\n",
    "        if not os.path.exists(thumbnail) or not os.path.isdir(thumbnail):\n",
    "            tqdm.write(colored(f\"Directory {thumbnail} not valid, create thumbnails first\", \"red\"))\n",
    "            self.directory = None\n",
    "            return\n",
    "        self.directory = directory\n",
    "        self.detectors = [\n",
    "            HashDetector(8),\n",
    "            HashDetector(16),\n",
    "            ORBDetector(500, 0.5),\n",
    "            ORBDetector(1000, 0.7)\n",
    "        ]\n",
    "\n",
    "    def deduplicate(self):\n",
    "        if self.directory is None:\n",
    "            tqdm.write(colored(\"Directory not valid\", \"red\"))\n",
    "            return\n",
    "        tqdm.write(colored(f\"Start deduplicating images in {self.directory}, collect thumbnail images.\", \"green\"))\n",
    "        thumbnails = [file for file in Path(f\"{self.directory}/thumbnail\").glob('*') if not file.name.startswith('.') and file.suffix.lower() in [\".jpg\", \".png\"]]\n",
    "        tqdm.write(colored(f\"Collected {len(thumbnails)} thumbnail images.\", \"green\"))\n",
    "        descriptor = ImageDescriptor(set(), [thumbnails])\n",
    "        previous_descriptor = None\n",
    "        prv_timestamp = None\n",
    "        prv_hash_list = []\n",
    "\n",
    "        for idx, detector in enumerate(self.detectors):\n",
    "            # tqdm.write(colored(f\"Process with {type(detector).__name__}[{id(detector)}], unique images: {len(descriptor.unique_images)} similar_groups count: {len(descriptor.similar_groups)}\", \"yellow\"))\n",
    "            new_descriptor = ImageDescriptor(descriptor.unique_images, [])\n",
    "\n",
    "            if False: #isinstance(detector, ORBDetector):\n",
    "                for group_of_img in tqdm(descriptor.similar_groups):\n",
    "                    result = detector.detect(group_of_img)\n",
    "                    new_descriptor.unique_images.update(result.unique_images)\n",
    "                    new_descriptor.similar_groups.extend(result.similar_groups)\n",
    "            else:\n",
    "                for group_of_img in descriptor.similar_groups:\n",
    "                    result = detector.detect(group_of_img)\n",
    "                    new_descriptor.unique_images.update(result.unique_images)\n",
    "                    new_descriptor.similar_groups.extend(result.similar_groups)\n",
    "            tqdm.write(colored(f\"Processed with {type(detector).__name__}[{id(detector)}], unique images: {len(new_descriptor.unique_images)} similar_groups count: {len(new_descriptor.similar_groups)}\", \"yellow\"))\n",
    "            hash_list = [hash(f\"{group[0]}{group[1]}{len(group)}\") for group in new_descriptor.similar_groups]\n",
    "            if previous_descriptor:\n",
    "                for group_idx, prev_hash in enumerate(prev_hash_list):\n",
    "                    if prev_hash not in hash_list:\n",
    "                        tqdm.write(colored(f\"Group {group_idx} is removed by {type(detector).__name__}\", \"blue\"))\n",
    "                        previous_descriptor.group_removed_by_next_detector.append(True)\n",
    "                    else:\n",
    "                        previous_descriptor.group_removed_by_next_detector.append(False)\n",
    "\n",
    "                # 使用上一个detector的名字来命名文件\n",
    "                filepath = f\"{self.directory}/descriptor_{idx-1}_{type(self.detectors[idx - 1]).__name__}_{prv_timestamp}.txt\"\n",
    "                previous_descriptor.serialize(filepath)\n",
    "\n",
    "            # 更新previous变量\n",
    "            previous_descriptor = new_descriptor\n",
    "            prv_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            prev_hash_list = hash_list\n",
    "            descriptor = new_descriptor\n",
    "\n",
    "        # 序列化最后一个descriptor\n",
    "        if previous_descriptor:\n",
    "            for idx in range(len(previous_descriptor.similar_groups)):\n",
    "                previous_descriptor.group_removed_by_next_detector.append(False)\n",
    "            filepath = f\"{self.directory}/descriptor_{len(self.detectors)-1}_{type(self.detectors[-1]).__name__}_{prv_timestamp}.txt\"\n",
    "            previous_descriptor.serialize(filepath)\n",
    "        \n",
    "        return descriptor\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# deduplicator = ImageDeduplicator(\"/Volumes/192.168.1.173/pic/陈都灵_503[167_MB]\")\n",
    "deduplicator = ImageDeduplicator(\"/Users/chenweichu/dev/data/test_副本\")\n",
    "\n",
    "final_descriptor = deduplicator.deduplicate()\n",
    "if final_descriptor is None:\n",
    "    tqdm.write(colored(\"Deduplication failed\", \"red\"))\n",
    "else:\n",
    "    tqdm.write(colored(f\"Final unique images count: {len(final_descriptor.unique_images)}\", \"white\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-img-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
