你是一位经验丰富的软件工程师。
你的代码质量很高，可读性强，易于维护，遵循了良好的代码规范，有详细的注释（主要以中文书写）。
你的代码有很高的鲁棒性，充分考虑到了可能遇到的各种异常情况并对每种情况都做了处理，包括但不限于空指针、数组越界、类型转换错误、IO异常等。
你的代码在性能方面也有良好的表现，能够高效地处理大量数据。
你的代码在各个关键步骤上都有日志输出，以便在出现问题时能够快速定位问题。

你将根据我提供的代码和新的需求，编写一段代码，并确保这段代码符合上述要求。

你编写这段代码的最终目的是：

[[[
输入一个目录，给目录中所有的图像文件去重。使用两种判断图像重复的算法：哈希值比较法和ORB特征距离法。
使用面向对象的方式编码。有如下对象：
1、相似图像组，由至少两张图片组成。
2、描述，由一个已被判明是唯一图像的列表和一个相似图像组对象的列表组成。
3、重复图像判断器，这个判断器可能是ORB特征判断器也可能是哈希值判断器。每个判断器有其属性来定义不同算法需要的阈值或精度。
比如哈希判断器有哈希精度的属性，如果是8，则图像转换为哈希值时会以8*8的大小计算。又比如ORB特征判断器有阈值属性，有生成ORB特征点数量属性。
可以传入一组图片给判断器，判断器会输出一个描述对象，表示这个判断器认为哪些图像是唯一的，哪些图像是相似的。

算法大致如下：
1、创建判断器对象列表，列表中首先包含N个哈希判断器对象，他们的精度从8开始以2的倍数递增，第一个是8，第二个是16，以此类推。列表中还包含N个ORB判断器，阈值、生成特征点数量依次提高，即他们对重复图像的判断越来越严格。
2、创建出目录的描述对象，此时应该所有图片都在一个相似图像组里，唯一图像列表为空。
3、循环，直至应用完所有的判断器。对于每一次循环做如下操作：
    【缩进1】取得当前判断器对象。
    【缩进1】遍历目录描述对象的相似图像列表，每一项做如下操作：
        【缩进2】对相似图像组的每个元素应用当前判断器，即传入相似图像组的所有图片给判断器进行是否重复的判断。输出的描述对象存入一个临时描述对象列表。
        【缩进2】将此相似图像组列表项从目录描述对象的相似图片组列表中删除。
    【缩进1】把临时描述对象列表的所有项与目录的描述对象合并，合并规则是：唯一图像列表合并，相似图像组列表合并。合并后输出目录描述对象中唯一图像列表的长度。
    【缩进1】当前判断器标记为已应用。
4、经过上面的循环，得到了一个最终的目录的描述对象，这个对象应该可以表示该目录下哪些图片是唯一的，哪些图片是互相重复的。输出这个描述对象。
]]]

你会在以下代码（基础代码）的基础上做修改来实现新的需求：

[[[
from PIL import Image
import numpy as np
import cv2
import os
from pathlib import Path
from imagehash import phash
from itertools import combinations
from typing import List, Set, Tuple
from termcolor import colored

class ImageDescriptor:
    def __init__(self, unique_images: Set[Path], similar_groups: List[List[Path]]):
        self.unique_images = unique_images
        self.similar_groups = similar_groups

class HashDetector:
    def __init__(self, precision: int):
        self.precision = precision

    def detect(self, images: List[Path]) -> ImageDescriptor:
        print(f"Detecting duplicates using perceptual hash, precision: {self.precision}\nimages cnt: {len(images)}")
        hash_dict = {}
        for image_path in images:
            with Image.open(image_path) as img:
                img_hash = phash(img.convert("L").resize((self.precision, self.precision)))
                if img_hash in hash_dict:
                    hash_dict[img_hash].append(image_path)
                else:
                    hash_dict[img_hash] = [image_path]
        print(f"Found {len(hash_dict)} unique hashes")
        unique_images = set()
        similar_groups = []
        for paths in hash_dict.values():
            if len(paths) == 1:
                unique_images.add(paths[0])
            else:
                similar_groups.append(paths)
        print(f"Found {len(unique_images)} unique images, {len(similar_groups)} similar groups")
        return ImageDescriptor(unique_images, similar_groups)

class ORBDetector:
    def __init__(self, nfeatures: int, threshold: float):
        self.nfeatures = nfeatures
        self.threshold = threshold

    def detect(self, images: List[Path]) -> ImageDescriptor:
        keypoints_dict = {img: self._extract_features(img) for img in images}
        similar_groups = []
        unique_images = set(images)
        
        for img1, img2 in combinations(images, 2):
            kp1, des1 = keypoints_dict[img1]
            kp2, des2 = keypoints_dict[img2]
            if des1 is not None and des2 is not None:
                if self._match_features(des1, des2) > self.nfeatures * self.threshold:
                    similar_groups.append([img1, img2])
                    if img1 in unique_images:
                        unique_images.remove(img1)
                    if img2 in unique_images:
                        unique_images.remove(img2)

        return ImageDescriptor(unique_images, similar_groups)

    def _extract_features(self, image_path: Path):
        orb = cv2.ORB_create(self.nfeatures)
        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        return orb.detectAndCompute(img, None)

    def _match_features(self, des1, des2):
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)
        return len(matches)

class ImageDeduplicator:
    def __init__(self, directory: str):
        self.directory = directory
        self.detectors = [
            HashDetector(8), # HashDetector(16),
            ORBDetector(500, 0.7), ORBDetector(1000, 0.8)
        ]

    def deduplicate(self):
        images = [path for path in Path(self.directory).glob("*") if path.suffix.lower() in [".jpg", ".png"]]
        descriptor = ImageDescriptor(set(), [images])

        for detector in self.detectors:
            print(colored(f"Processed with {type(detector).__name__}[{id(detector)}], similar_groups count: {len(descriptor.similar_groups)}", "yellow"))
            new_descriptor = ImageDescriptor(descriptor.unique_images, [])
            for group in descriptor.similar_groups:
                print(f"Processing group of size {len(group)}")
                result = detector.detect(group)
                new_descriptor.unique_images |= result.unique_images
                new_descriptor.similar_groups.extend(result.similar_groups)
            descriptor = new_descriptor
            print(colored(f"unique count: {len(descriptor.unique_images)}", "yellow"))
        
        return descriptor

# Example usage
deduplicator = ImageDeduplicator("/Users/chenweichu/dev/data/test")
final_descriptor = deduplicator.deduplicate()
print(f"Final unique images count: {len(final_descriptor.unique_images)}")
]]]

新的需求是：

[[[
给描述类增加一个序列化的方法，可以传入文件路径序列化为文本文件。记录的所有图片只需要文件名和扩展名，不需要保存绝对路径。
在这个循环：for detector in self.detectors:里，每次更新了描述类后，都将描述类序列化到工作目录，文件名里需要有时间戳和这次循环使用的判断器的名字。
]]]

你只需要在上面代码的基础上做尽量少的修改来解决问题，对于可以不用修改的部分不进行修改。
不要试图删除基础代码中的注释和用于调试的日志输出，除非这是必要的。
最后，给我展示修改后用于替换基础代码的完整代码（最终代码），包括修改了的部分和未修改的部分。
给我的最终代码应当是完整可运行的。
最终代码不应该包含任何省略或指向性注释。
不应该包含类似于“...省略实现”、“...保持不变”、“与...相同”这样的注释来代替实际代码。