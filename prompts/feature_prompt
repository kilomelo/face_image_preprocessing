你是一位经验丰富的软件工程师。
你的代码质量很高，可读性强，易于维护，遵循了良好的代码规范，有详细的注释（主要以中文书写）。
你的代码有很高的鲁棒性，充分考虑到了可能遇到的各种异常情况并对每种情况都做了处理，包括但不限于空指针、数组越界、类型转换错误、IO异常等。
你的代码在性能方面也有良好的表现，能够高效地处理大量数据。

你将根据我提出的需求，编写一段代码，并确保这段代码符合上述要求。

这是需求，包括过往历次迭代的需求记录：
[[[
输入一个目录，给目录中所有的图像文件去重。使用两种判断图像重复的算法：哈希值比较法和ORB特征距离法。

此次修改的新需求:
    [[
        在这个循环中：
        [for detector in self.detectors:]
        获得descriptor后[descriptor = new_descripter]，对descriptor进行操作，获得similar_groups中每个元素的哈希值。并以列表的形式保存在descriptor中。我们称为哈希列表。
        不要在每次循环逻辑结束时序列化这次循环得到的描述对象，因为还需要根据下个detector的执行结果对这个描述对象进行更新。
        下个detector执行完毕并获得descriptor，且计算完哈希列表后，遍历上次循环得到的descriptor的哈希列表，如果哈希值存在于这次循环的哈希列表，则标该哈希值对应的上次循环得到的descriptor中的那个group为“已破坏”。
        完成这个操作后，就可以序列化上次循环得到的Descriptor对象了。
        注意,这样的操作会发生在循环里除了最后一个元素以外的所有元素,最后一个元素产生的descripor无需进行这样的哈希值比对,可以直接序列化。
        
        descriptor序列化的逻辑也进行修改:
        对于标记为“已破坏”的group，这里写入时[file.write(f"Group:\n")],在‘:’后增加“removed”
    ]]
]]]
这是目前已实现的代码：
[[[
# 迭代编号：1
from PIL import Image
import numpy as np
import cv2
import os
from pathlib import Path
from imagehash import phash
from itertools import combinations
from typing import List, Set, Tuple
from termcolor import colored
import datetime
from tqdm import tqdm

class ImageDescriptor:
    def __init__(self, unique_images: Set[Path], similar_groups: List[List[Path]]):
        self.unique_images = unique_images
        self.similar_groups = similar_groups
        # tqdm.write(colored(f"ImageDescripter constructed: {[len(unique_images)] + [len(sub_array) for sub_array in similar_groups]}", "white"))

    def serialize(self, filepath: str):
        """将描述信息保存为文本文件"""
        tqdm.write(colored(f"Saving description to {filepath}", "white"))
        with open(filepath, 'w') as file:
            file.write("Unique Images:\n")
            for image in self.unique_images:
                file.write(f"{image.name}\n")
            file.write("\nSimilar Groups:\n")
            for group in self.similar_groups:
                file.write(f"Group:\n")
                for image in group:
                    file.write(f"{image.name}\n")
                file.write("\n")

class HashDetector:
    def __init__(self, precision: int):
        self.precision = precision

    def detect(self, images: List[Path]) -> ImageDescriptor:
        # tqdm.write(colored(f"Detecting duplicates using perceptual hash, precision: {self.precision}\nimages cnt: {len(images)}", "white"))
        hash_dict = {}
        
        # 添加tqdm进度条
        for image_path in tqdm(images, desc="Hashing images"):
            try:
                with Image.open(image_path) as img:
                    # 计算图片的perceptual hash
                    img_hash = phash(img.convert("L").resize((self.precision, self.precision)))
                    if img_hash in hash_dict:
                        hash_dict[img_hash].append(image_path)
                    else:
                        hash_dict[img_hash] = [image_path]
            except Exception as e:
                tqdm.write(colored(f"Error processing {image_path}: {e}", "red"))
                    
        tqdm.write(colored(f"Found {len(hash_dict)} unique hashes", "white"))
        unique_images = set()
        similar_groups = []
        
        # 添加tqdm进度条
        for paths in tqdm(hash_dict.values(), desc="Grouping images"):
            if len(paths) == 1:
                unique_images.add(paths[0])
            else:
                similar_groups.append(paths)
                
        tqdm.write(colored(f"Found {len(unique_images)} unique images, {len(similar_groups)} similar groups", "white"))
        return ImageDescriptor(unique_images, similar_groups)

class ORBDetector:
    def __init__(self, nfeatures: int, threshold: float):
        self.nfeatures = nfeatures
        self.threshold = threshold

    def detect(self, images: List[Path]) -> ImageDescriptor:
        # tqdm.write(colored(f"Detecting duplicates using ORB, nfeatures: {self.nfeatures}, threshold: {self.threshold}, images cnt: {len(images)}", "white"))
        keypoints_dict = {img: self._extract_features(img) for img in images}
        similar_groups = []
        unique_images = set(images)
        
        # 直接计算组合数，而不生成组合列表
        combines = combinations(images, 2)
        
        # 使用tqdm直接包装组合迭代器
        # for img1, img2 in tqdm(combines, total=len(images) * (len(images) - 1) // 2, desc="Matching features"):
        for img1, img2 in combines:
            kp1, des1 = keypoints_dict[img1]
            kp2, des2 = keypoints_dict[img2]
            if des1 is not None and des2 is not None:
                if self._match_features(des1, des2) > self.nfeatures * self.threshold:
                    similar_groups.append([img1, img2])
                    unique_images.discard(img1)
                    unique_images.discard(img2)

        return ImageDescriptor(unique_images, similar_groups)

    def _extract_features(self, image_path: Path):
        orb = cv2.ORB_create(self.nfeatures)
        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        return orb.detectAndCompute(img, None)

    def _match_features(self, des1, des2):
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)
        return len(matches)

class ImageDeduplicator:
    def __init__(self, directory: str):
        if not os.path.exists(directory) or not os.path.isdir(directory):
            tqdm.write(colored(f"Directory {directory} not valid", "red"))
            self.directory = None
            return
        thumbnail = Path(f"{directory}/thumbnail")
        if not os.path.exists(thumbnail) or not os.path.isdir(thumbnail):
            tqdm.write(colored(f"Directory {thumbnail} not valid, create thumbnails first", "red"))
            self.directory = None
            return
        self.directory = directory
        self.detectors = [
            HashDetector(8),
            HashDetector(16),
            ORBDetector(500, 0.5),
            ORBDetector(1000, 0.7)
        ]

    def deduplicate(self):

        if self.directory is None:
            tqdm.write(colored("Directory not valid", "red"))
            return
        thumbails = [file for file in Path(f"{self.directory}/thumbnail").glob('*') if not file.name.startswith('.') and file.suffix.lower() in [".jpg", ".png"]]
        descriptor = ImageDescriptor(set(), [thumbails])

        for detector in self.detectors:
            tqdm.write(colored(f"Processed with {type(detector).__name__}[{id(detector)}], similar_groups count: {len(descriptor.similar_groups)}", "yellow"))
            new_descripter = ImageDescriptor(descriptor.unique_images, [])
            if isinstance(detector, ORBDetector):
                for group_of_img in tqdm(descriptor.similar_groups):
                    result = detector.detect(group_of_img)
                    new_descripter.unique_images.update(result.unique_images)
                    new_descripter.similar_groups.extend(result.similar_groups)
            else:
                for group_of_img in descriptor.similar_groups:
                    result = detector.detect(group_of_img)
                    new_descripter.unique_images.update(result.unique_images)
                    new_descripter.similar_groups.extend(result.similar_groups)
            descriptor = new_descripter
            # 序列化待序列化的描述对象
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
            filepath = f"{self.directory}/descriptor_{type(detector).__name__}_{timestamp}.txt"
            descriptor.serialize(filepath)
            tqdm.write(colored(f"After processed, similar_groups count: {len(descriptor.similar_groups)}, unique count: {len(descriptor.unique_images)}\n", "yellow"))
        
        return descriptor

# Example usage
# deduplicator = ImageDeduplicator("/Users/chenweichu/dev/data/test_副本")
deduplicator = ImageDeduplicator("/Volumes/192.168.1.173/pic/陈都灵_503[167_MB]")
deduplicator = ImageDeduplicator("/Volumes/192.168.1.173/pic/鞠婧祎_4999[5_GB]")

final_descriptor = deduplicator.deduplicate()
if final_descriptor is None:
    tqdm.write(colored("Deduplication failed", "red"))
else:
    tqdm.write(colored(f"Final unique images count: {len(final_descriptor.unique_images)}", "white"))
]]]

修改代码过程中严格遵守以下几条规则：
1、禁止修改任何和新增需求无关的部分！
2、所有的方法(method)需要添加详细的注释，包括方法功能，参数、返回值的解释。
3、每一句代码都需要有注释，以提高代码的可读性。
4、输出尽量多的日志，以使调试过程中能获取更多的状态信息，提高调试效率。
5、禁止移除旧代码中的功能，除非这是上文中“此次修改的新需求”里明确要求的。
6、禁止删除旧代码中被注释掉的代码。

对于迭代编号的标注；
1、旧代码的迭代编号见代码第一行。
2、此次修改的迭代编号在旧代码中的迭代编号上加1。修改后的代码的第一行显示此次修改的迭代编号。
3、此次迭代做出的每一处代码改动都需要用注释标注，标明此次修改的迭代编号。格式为：(改动描述) 迭代编号：(此次改动的迭代编号)。
4、如果旧代码中有包含迭代编号的注释（非此次迭代编号），删除掉迭代编号，保留对代码解释的部分。

最后，展示修改后用于的完整代码，完整代码应该严格满足以下要求：
1、是可运行的。
2、包含此次修改的部分和未修改的部分。
3、不包含任何省略或指向性注释。
4、不应该包含类似于“...省略实现”、“...保持不变”、“与...相同”这样的注释来代替实际代码。